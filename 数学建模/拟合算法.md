# 拟合算法

## 插值与拟合的区别

插值算法中，得到的多项式$f(x)$要经过所有样本点。但如果样本点太多，那么这个多项式次数过高，会造成龙格现象。

采用拟合算法可以让尽量多的样本点在我们预测的曲线上，尽管不能保证所有样本点都在曲线上，但只要保证误差足够小即可。这就是拟合的思想。

## 最小二乘法

### 最小二乘法的几何解释

设样本点为$(x_i,y_i),i=1,2,\dots,n$设置的拟合曲线为$y=kx+b$.问题：$k$和$b$取何值时，样本点和拟合曲线最接近。

第一种定义：
$$
\hat y_i = kx_i+b\\
\hat k, \hat b = argmin(\sum_{i=1}^n|y_i-\hat y_i|)
$$
第二种定义：
$$
\hat y_i = kx_i+b\\
\hat k, \hat b = argmin(\sum_{i=1}^n(y_i-\hat y_i)^2)
$$
第一种定义有绝对值，不容易求导，因此计算比较复杂。所以我们往往采用第二种定义，这也是最小二乘的思想。

### 求解最小二乘法

令$L=\sum_{i=1}^n(y_i-kx_i-b)^2$，现要找$k,b$使得$L$最小
$$
\begin{cases}
{{\partial L}\over{\part}k}=-2\sum_{i=1}^nx_i(y_i-kx_i-b)=0
\\&\Rightarrow
\\{{\partial L}\over{\part}b}=-2\sum_{i=1}^n(y_i-kx_i-b)=0 
\end{cases}
\begin{cases}
\sum_{i=1}^nx_iy_i=k\sum_{i=1}^nx_i^2+b\sum_{i=1}^nx_i\\
& \Rightarrow\\
\sum_{i=1}^ny_i=k\sum_{i=1}^nx_i+bn
\end{cases}
$$ {\\}

$$
\begin{cases}
n\sum_{i=1}^nx_iy_i=kn\sum_{i=1}^nx_i^2+bn\sum_{i=1}^nx_i\\
\sum_{i=1}^ny_i \sum_{i=1}^nx_i=k\sum_{i=1}^nx_i\sum_{i=1}^n+bn\sum_{i=1}^nx_i
\end{cases}
$$

$$
n\sum_{i=1}^nx_iy_i-\sum_{i=1}^ny_i\sum_{i=1}^nx_i=kn\sum_{i=1}^nx_i^2-k\sum_{i=1}^nx_i\sum_{i=1}^nx_i \\
\Rightarrow \hat k={{n\sum_{i=1}^nx_iy_i-\sum_{i=1}^ny_i\sum_{i=1}^nx_i}\over n\sum_{i=1}^nx_i^2-\sum_{i=1}^nx_i\sum_{i=1}^nx_i}
$$

同理可得：
$$
\hat b={{\sum_{i=1}^nx_i^2\sum_{i=1}^ny_i-\sum_{i=1}^nx_i\sum_{i=1}^nx_iy_i}\over n\sum_{i=1}^nx_i^2-\sum_{i=1}^nx_i\sum_{i=1}^nx_i}
$$

## 拟合优度

用来评价拟合程度的好坏

拟合优度（可决系数）$R^2$

总体平方和$SST:Total\ sum\ of\ squares:SST=\sum_{i=1}^n(y_i-\bar y)^2$

误差平方和$SSE: The \ sum \ of \ squares\ due\ to\ error:SSE=\sum_{i=1}^n(y_i-\hat y_i)^2$

回归平方和$SSR:Sum\ of\ squares\ of\ the\ regression:SSR=\sum_{i=1}^n(\hat y_i - \bar y)^2$

可以证明：$SST=SSE+SSR$

拟合优度：
$$
0 \le R^2 = {SSR\over SST} = {{SST-SSE}\over SST}=1-{SSE\over SST}\le 1
$$
$R^2$越接近1，说明误差平方和越接近0，误差越小说明拟合越好。

> $R^2$只能用于拟合函数是线性函数时拟合结果的评价。
>
> 线性函数和其它函数（例如指数函数）比较拟合的好坏，直接看SSE即可
>
> 线性函数的含义是对参数时线性的，在函数中，假如参数不是以一次方出现或乘以或除以其他的参数，或出现复合函数形式的，都不是线性函数。